{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практическое задание к уроку 3. Построение модели классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Для чего и в каких случаях полезны различные варианты усреднения для метрик качества классификации: micro, macro, weighted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применяется в многоклассовых задачах, для получения итоговой метрики качества по всем классам. Выделяют три\n",
    "подхода к такому сведению:micro, macro, weighted-усреднение.\n",
    "\n",
    "При микро-усреднении сначала эти характеристики усредняются по всем классам, а\n",
    "затем вычисляется итоговая метрика — например, точность, полнота или F-мера (Если классы отличаются по мощности, то при микро-усреднении они практически никак не будут влиять на результат, поскольку их вклад в средние TP, FP, FN и\n",
    "TN будет незначителен). \n",
    "\n",
    "При макро-усреднении сначала вычисляется итоговая метрика для каждого класса, а затем результаты усредняются по всем классам (усреднение проводится для\n",
    "величин, которые уже не чувствительны к соотношению размеров классов, и поэтому\n",
    "каждый класс внесет равный вклад в итоговую метрику).\n",
    "\n",
    "\n",
    "При weighted-усреднении сначала вычисляется метрики потом находится их средний вес в выборке. Это позваляет учесть дисбаланс классов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. В чём разница между моделями xgboost, lightgbm и catboost или какие их основные особенности?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Xgboost\n",
    "\n",
    "Самая распространенная реализация градиентного бустинга.  Для выбора разбиения используют сортировку и модели, основанные на анализе гистограмм. не работает с категориальными переменными \n",
    "\n",
    "\n",
    "#### LightGBM\n",
    "\n",
    "Для выбора критерия разбиения используется Gradient-based One-Side Sampling (GOSS)(Основное предположение, принятое здесь, состоит в том, что выборки с обучающими экземплярами с небольшими градиентами имеют меньшую ошибку обучения, и она уже хорошо обучена.Чтобы сохранить то же распределение данных, при вычислении прироста информации GOSS вводит постоянный множитель для экземпляров данных с небольшими градиентами. Таким образом, GOSS обеспечивает хороший баланс между уменьшением количества экземпляров данных и сохранением точности для выученных деревьев решений.). \n",
    "Имеются методы работы с категориальными признаками, т.е. с признаками, которые явно не выражаются числом (например, имя автора или марка машины).\n",
    "\n",
    "\n",
    "#### CatBoost\n",
    "\n",
    "Реализует особый подход к обработке категориальных признаков (основанный на target encoding, т.е. на подмене категориальных признаков статистиками на основе предсказываемого значения). Механизм построения и отбора деревьев решений (строится одно дерево. определение  качества. проверка переобучения, строится лес алгоритмом  перебора деревьев с лучшим результатом и пересчитывается значение дерево и т.д)\n",
    " \n",
    " По скорости работы считается что Xgboost является наиболее медленным"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
